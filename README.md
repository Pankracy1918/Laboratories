Program created to work on laboratories related to scientometrics. Helps to visualize what words are most common in scientific articles.

The program first takes as input any pdf article, then transform it into string. After this the process of word tokenization starts. The next step is deleting the punctuacion signs. Last step of data cleaning is getting rid of stopwrods with help of nlt corpus module. And the final step is visualization.
